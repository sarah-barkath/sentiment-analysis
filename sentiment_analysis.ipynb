{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70084938",
   "metadata": {},
   "source": [
    "# Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd072805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8eec7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b01a27-27ce-451d-bf2b-ad2bdc918cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0355a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Sarah Barkath\\sentiment_analysis\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faac3cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sarah Barkath\\sentiment_analysis\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Sarah Barkath\\sentiment_analysis\\venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#tokenization using given model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model_tf = TFAutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc772100",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "config = AutoConfig.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3f0a5b",
   "metadata": {},
   "source": [
    "Generic code for model pipeline and tokenization ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d56b5cf-cd61-41d2-ad32-5c80ba90dc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           username                                             review\n",
      "0     FoodieFrank89  Cozy Café charms with its intimate setting and...\n",
      "1   CaffeineLover23  Cozy Corner Café is my go-to for a coffee fix ...\n",
      "2           sarah_b  Cozy Corner Café offers nutritious options lik...\n",
      "3       GourmetGuru  Cozy Corner Café impresses with its gourmet fl...\n",
      "4       monica_john  Cozy Corner Café fell short of expectations. T...\n",
      "5          susan123  Cozy Corner Café is our family favorite! The k...\n",
      "6      samantha0989  Indulge in Cozy Corner Café's dessert heaven! ...\n",
      "7           lewis_p  Cozy Corner Café caters brilliantly to vegetar...\n",
      "8         hugo_boss  Cozy Corner Café lacked flavor in their dishes...\n",
      "9      diane_gordan  Cozy Corner Café needs to work on their servic...\n",
      "10              tom  Cozy Corner Café disappointed with cold, unins...\n",
      "11            jerry  Service at Cozy Corner Café was slow; waited 4...\n",
      "12             nick  Cozy Corner Café's vegetarian options were lim...\n",
      "13            peter  Cozy Corner Café is overpriced for what you ge...\n",
      "14            nancy  Tables at Cozy Corner Café were dirty, and it ...\n",
      "15            becky  Cozy Corner Café was too noisy for a relaxed m...\n",
      "16          roxanne  Cozy Corner Café was too noisy for a relaxed m...\n"
     ]
    }
   ],
   "source": [
    "#insert excel sheet for analysis of multiple user inputs\n",
    "def xl_to_df():\n",
    "    df=pd.read_excel('sample survey - new.xlsx')\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e9d6973e-04f2-4c76-a5a3-6a923e3c6fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review\n",
      "0   Cozy Café charms with its intimate setting and...\n",
      "1   Cozy Corner Café is my go-to for a coffee fix ...\n",
      "2   Cozy Corner Café offers nutritious options lik...\n",
      "3   Cozy Corner Café impresses with its gourmet fl...\n",
      "4   Cozy Corner Café fell short of expectations. T...\n",
      "5   Cozy Corner Café is our family favorite! The k...\n",
      "6   Indulge in Cozy Corner Café's dessert heaven! ...\n",
      "7   Cozy Corner Café caters brilliantly to vegetar...\n",
      "8   Cozy Corner Café lacked flavor in their dishes...\n",
      "9   Cozy Corner Café needs to work on their servic...\n",
      "10  Cozy Corner Café disappointed with cold, unins...\n",
      "11  Service at Cozy Corner Café was slow; waited 4...\n",
      "12  Cozy Corner Café's vegetarian options were lim...\n",
      "13  Cozy Corner Café is overpriced for what you ge...\n",
      "14  Tables at Cozy Corner Café were dirty, and it ...\n",
      "15  Cozy Corner Café was too noisy for a relaxed m...\n",
      "16  Cozy Corner Café was too noisy for a relaxed m...\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "#inserting all sentences into a data frame\n",
    "def one_col(df):\n",
    "    df_new = df.iloc[:,[1]]\n",
    "    print(df_new)\n",
    "    print(len(df_new))\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0d85e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one user input + adding into data frame\n",
    "df_new = pd.DataFrame()\n",
    "def sentence_input():\n",
    "    user_input = input(\"Enter a string value: \")\n",
    "    df = pd.DataFrame({'sentence': [user_input]})\n",
    "    print(df)\n",
    "    df_new = df\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d6f7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes all special characters\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdf84d94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#tokenization; gives an array of tensor(s)\n",
    "\n",
    "inputs = []\n",
    "def token_input_total(dataframe):\n",
    "    ip=[]\n",
    "    for i in dataframe:\n",
    "        text = i\n",
    "        text = preprocess(text)\n",
    "        input = tokenizer(text, return_tensors=\"pt\")\n",
    "        ip.append(input)\n",
    "    return ip\n",
    "inputs = token_input_total(df_new)\n",
    "print(inputs)\n",
    "\n",
    "#row-wise sentiment analysis\n",
    "#inputs = []\n",
    "#def token_input(df_new):\n",
    "#    ip=[]\n",
    "#    for i, row in df_new.iterrows():\n",
    "#        text = row[0]\n",
    "#        text = preprocess(text)\n",
    "#        input = tokenizer(text, return_tensors=\"pt\")\n",
    "#        ip.append(input)\n",
    "#    return ip\n",
    "#inputs = token_input(df_new)\n",
    "#print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c05b97ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#row-wise\n",
    "\n",
    "inputs = []\n",
    "def token_input(dataframe):\n",
    "    ip=[]\n",
    "    for i, row in dataframe.iterrows():\n",
    "        text = row[0]\n",
    "        text = preprocess(text)\n",
    "        input = tokenizer(text, return_tensors=\"pt\")\n",
    "        ip.append(input)\n",
    "    return ip\n",
    "inputs = token_input(df_new)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4150ed44-7efb-45cf-be89-868af1b7a44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#calculates logits for sentence(s)\n",
    "\n",
    "logits_list=[]\n",
    "def logit_calc(inputs):\n",
    "    for i in inputs:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**i)\n",
    "            logits = outputs.logits\n",
    "            logits_list.append(logits)\n",
    "    return logits_list\n",
    "logits = logit_calc(inputs)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba530f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing sentiment results\n",
    "\n",
    "def final_results(prob):\n",
    "    scores = prob[0].detach().numpy()\n",
    "    s = scores*100\n",
    "    print(s)\n",
    "    ranking = np.argsort(s)\n",
    "    ranking = ranking[::-1] #descending\n",
    "    for i in range(s.shape[0]):\n",
    "        l = config.id2label[ranking[i]]  #0-negative 1-neutral 2-positive\n",
    "        sc = s[ranking[i]]\n",
    "        print(f\"{i+1}) {l} {np.round(float(sc), 4)}\") #descending order of labelled scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e99a7a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates probabilities of given logits\n",
    "\n",
    "def prob_op(logit):\n",
    "    probabilities = F.softmax(logit, dim=1)\n",
    "    return probabilities\n",
    "p =[]\n",
    "for i in logits:\n",
    "    prob = prob_op(i)\n",
    "    p = p + [prob]\n",
    "    final_results(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all functions called here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_analysis_kernel",
   "language": "python",
   "name": "sentiment_analysis_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
